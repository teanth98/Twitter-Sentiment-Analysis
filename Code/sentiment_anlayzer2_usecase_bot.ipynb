{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use-Case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_collection = []\n",
    "for dirname, _, filenames in os.walk('C:\\\\Users\\skrPlays\\Downloads\\\\tweets'):\n",
    "    for filename in filenames:\n",
    "        fullpath= os.path.join(dirname, filename)\n",
    "        csv_collection.append(fullpath)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skrPlays\\AppData\\Local\\Temp\\ipykernel_16764\\2102135557.py:9: DtypeWarning: Columns (18,21,24,25,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  tmp = pd.read_csv(data, compression = 'gzip', index_col=0,\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(csv_collection.pop(),compression = 'gzip', index_col=0,\n",
    "                 dtype={'userid':int, 'username':object, 'acctdesc':object, 'location':object,\n",
    "                        'following':int, 'followers':int, 'totaltweets':int, 'usercreatedts':object,\n",
    "                        'tweetid':int, 'tweetcreatedts':object, 'retweetcount':int, 'text':object,\n",
    "                        'hashtags':object, 'language':object, 'coordinates':object, 'favorite_count':int,\n",
    "                        'extractedts':object})\n",
    "for data in csv_collection:\n",
    "    try:\n",
    "        tmp = pd.read_csv(data, compression = 'gzip', index_col=0,\n",
    "                          dtype={'userid':int, 'username':object, 'acctdesc':object, 'location':object,\n",
    "                                 'following':int, 'followers':int, 'totaltweets':int, 'usercreatedts':object,\n",
    "                                 'tweetid':int, 'tweetcreatedts':object, 'retweetcount':int, 'text':object,\n",
    "                                 'hashtags':object, 'language':object, 'coordinates':object, 'favorite_count':int,\n",
    "                                 'extractedts':object})\n",
    "    except:\n",
    "        tmp = pd.read_csv(data, index_col = 0)\n",
    "    df = pd.concat([df, tmp], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "2022-03-07    567745\n2022-03-06    566767\n2022-03-05    546780\n2022-03-08    519385\n2022-03-09    493857\n               ...  \n2022-04-16    332860\n2022-04-17    325909\n2022-04-27    318809\n2022-04-25    317674\n2022-02-24    298045\nName: tweetcreatedts, Length: 63, dtype: int64"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df[[\"userid\", \"tweetcreatedts\", \"username\", \"text\", \"language\"]]\n",
    "data = data[data[\"language\"]==\"en\"]\n",
    "data.isnull().sum()\n",
    "data_dates =  pd.to_datetime(df['tweetcreatedts'],  errors='coerce', infer_datetime_format=True).dt.date\n",
    "data_dates.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\skrPlays\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "stopword=set(stopwords.words('english'))\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text=\" \".join(text)\n",
    "    return text\n",
    "data[\"tweet\"] = data[\"text\"].apply(clean)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "data['tweet_date'] =  pd.to_datetime(data['tweetcreatedts'],  infer_datetime_format=True).dt.date"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 800. MiB for an array with shape (6, 17484308) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [14]\u001B[0m, in \u001B[0;36m<cell line: 12>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      9\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m ids \u001B[38;5;129;01min\u001B[39;00m spam_series:\n\u001B[0;32m     10\u001B[0m             spam_id_list\u001B[38;5;241m.\u001B[39mappend(ids)\n\u001B[1;32m---> 12\u001B[0m \u001B[43mfindSpamBots\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m spam_id_list\n",
      "Input \u001B[1;32mIn [14]\u001B[0m, in \u001B[0;36mfindSpamBots\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m counts \u001B[38;5;241m=\u001B[39m freq\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m bt, c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(bot_tweets, counts):\n\u001B[1;32m----> 8\u001B[0m     spam_series \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtweet\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbt\u001B[49m\u001B[43m]\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muserid\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m ids \u001B[38;5;129;01min\u001B[39;00m spam_series:\n\u001B[0;32m     10\u001B[0m         spam_id_list\u001B[38;5;241m.\u001B[39mappend(ids)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Twitter-Sentiment-Analysis\\lib\\site-packages\\pandas\\core\\frame.py:3496\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3494\u001B[0m \u001B[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001B[39;00m\n\u001B[0;32m   3495\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m com\u001B[38;5;241m.\u001B[39mis_bool_indexer(key):\n\u001B[1;32m-> 3496\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_bool_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3498\u001B[0m \u001B[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001B[39;00m\n\u001B[0;32m   3499\u001B[0m \u001B[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001B[39;00m\n\u001B[0;32m   3500\u001B[0m is_single_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mtuple\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_list_like(key)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Twitter-Sentiment-Analysis\\lib\\site-packages\\pandas\\core\\frame.py:3551\u001B[0m, in \u001B[0;36mDataFrame._getitem_bool_array\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3549\u001B[0m key \u001B[38;5;241m=\u001B[39m check_bool_indexer(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex, key)\n\u001B[0;32m   3550\u001B[0m indexer \u001B[38;5;241m=\u001B[39m key\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m-> 3551\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_take_with_is_copy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Twitter-Sentiment-Analysis\\lib\\site-packages\\pandas\\core\\generic.py:3728\u001B[0m, in \u001B[0;36mNDFrame._take_with_is_copy\u001B[1;34m(self, indices, axis)\u001B[0m\n\u001B[0;32m   3720\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_take_with_is_copy\u001B[39m(\u001B[38;5;28mself\u001B[39m: NDFrameT, indices, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NDFrameT:\n\u001B[0;32m   3721\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   3722\u001B[0m \u001B[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001B[39;00m\n\u001B[0;32m   3723\u001B[0m \u001B[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3726\u001B[0m \u001B[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001B[39;00m\n\u001B[0;32m   3727\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 3728\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3729\u001B[0m     \u001B[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001B[39;00m\n\u001B[0;32m   3730\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m result\u001B[38;5;241m.\u001B[39m_get_axis(axis)\u001B[38;5;241m.\u001B[39mequals(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_axis(axis)):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Twitter-Sentiment-Analysis\\lib\\site-packages\\pandas\\core\\generic.py:3713\u001B[0m, in \u001B[0;36mNDFrame.take\u001B[1;34m(self, indices, axis, is_copy, **kwargs)\u001B[0m\n\u001B[0;32m   3704\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   3705\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_copy is deprecated and will be removed in a future version. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3706\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtake\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m always returns a copy, so there is no need to specify this.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3707\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m   3708\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m   3709\u001B[0m     )\n\u001B[0;32m   3711\u001B[0m nv\u001B[38;5;241m.\u001B[39mvalidate_take((), kwargs)\n\u001B[1;32m-> 3713\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_consolidate_inplace\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3715\u001B[0m new_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39mtake(\n\u001B[0;32m   3716\u001B[0m     indices, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_block_manager_axis(axis), verify\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   3717\u001B[0m )\n\u001B[0;32m   3718\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_constructor(new_data)\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtake\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Twitter-Sentiment-Analysis\\lib\\site-packages\\pandas\\core\\generic.py:5661\u001B[0m, in \u001B[0;36mNDFrame._consolidate_inplace\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   5658\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mf\u001B[39m():\n\u001B[0;32m   5659\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39mconsolidate()\n\u001B[1;32m-> 5661\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_protect_consolidate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Twitter-Sentiment-Analysis\\lib\\site-packages\\pandas\\core\\generic.py:5649\u001B[0m, in \u001B[0;36mNDFrame._protect_consolidate\u001B[1;34m(self, f)\u001B[0m\n\u001B[0;32m   5647\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f()\n\u001B[0;32m   5648\u001B[0m blocks_before \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39mblocks)\n\u001B[1;32m-> 5649\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5650\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39mblocks) \u001B[38;5;241m!=\u001B[39m blocks_before:\n\u001B[0;32m   5651\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clear_item_cache()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Twitter-Sentiment-Analysis\\lib\\site-packages\\pandas\\core\\generic.py:5659\u001B[0m, in \u001B[0;36mNDFrame._consolidate_inplace.<locals>.f\u001B[1;34m()\u001B[0m\n\u001B[0;32m   5658\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mf\u001B[39m():\n\u001B[1;32m-> 5659\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mgr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mgr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconsolidate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Twitter-Sentiment-Analysis\\lib\\site-packages\\pandas\\core\\internals\\managers.py:631\u001B[0m, in \u001B[0;36mBaseBlockManager.consolidate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    629\u001B[0m bm \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxes, verify_integrity\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    630\u001B[0m bm\u001B[38;5;241m.\u001B[39m_is_consolidated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m \u001B[43mbm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_consolidate_inplace\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m bm\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Twitter-Sentiment-Analysis\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1685\u001B[0m, in \u001B[0;36mBlockManager._consolidate_inplace\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1683\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_consolidate_inplace\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1684\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_consolidated():\n\u001B[1;32m-> 1685\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(\u001B[43m_consolidate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblocks\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1686\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_consolidated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   1687\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_known_consolidated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Twitter-Sentiment-Analysis\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2084\u001B[0m, in \u001B[0;36m_consolidate\u001B[1;34m(blocks)\u001B[0m\n\u001B[0;32m   2082\u001B[0m new_blocks: \u001B[38;5;28mlist\u001B[39m[Block] \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m   2083\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (_can_consolidate, dtype), group_blocks \u001B[38;5;129;01min\u001B[39;00m grouper:\n\u001B[1;32m-> 2084\u001B[0m     merged_blocks \u001B[38;5;241m=\u001B[39m \u001B[43m_merge_blocks\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2085\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mgroup_blocks\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcan_consolidate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_can_consolidate\u001B[49m\n\u001B[0;32m   2086\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2087\u001B[0m     new_blocks \u001B[38;5;241m=\u001B[39m extend_blocks(merged_blocks, new_blocks)\n\u001B[0;32m   2088\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m new_blocks\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Twitter-Sentiment-Analysis\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2118\u001B[0m, in \u001B[0;36m_merge_blocks\u001B[1;34m(blocks, dtype, can_consolidate)\u001B[0m\n\u001B[0;32m   2115\u001B[0m     new_values \u001B[38;5;241m=\u001B[39m bvals2[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39m_concat_same_type(bvals2, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m   2117\u001B[0m argsort \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margsort(new_mgr_locs)\n\u001B[1;32m-> 2118\u001B[0m new_values \u001B[38;5;241m=\u001B[39m \u001B[43mnew_values\u001B[49m\u001B[43m[\u001B[49m\u001B[43margsort\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m   2119\u001B[0m new_mgr_locs \u001B[38;5;241m=\u001B[39m new_mgr_locs[argsort]\n\u001B[0;32m   2121\u001B[0m bp \u001B[38;5;241m=\u001B[39m BlockPlacement(new_mgr_locs)\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 800. MiB for an array with shape (6, 17484308) and data type object"
     ]
    }
   ],
   "source": [
    "spam_id_list = []\n",
    "def findSpamBots():\n",
    "    freq = data.tweet.value_counts()[0:10]\n",
    "    bot_tweets = freq.index\n",
    "    counts = freq.values\n",
    "\n",
    "    for bt, c in zip(bot_tweets, counts):\n",
    "        spam_series = data[data.tweet == bt]['userid']\n",
    "        for ids in spam_series:\n",
    "            spam_id_list.append(ids)\n",
    "\n",
    "findSpamBots()\n",
    "spam_id_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data['bot_flag']=data['userid'].apply(lambda x: 1 if x in(spam_id_list) else 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "(data.groupby('tweet_date')['bot_flag'].value_counts(normalize=True).unstack()).to_csv(\"bot_en.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}